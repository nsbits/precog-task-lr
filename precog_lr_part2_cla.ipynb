{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hEg8aL-Yw147"
      },
      "outputs": [],
      "source": [
        "# ==== 0) Setup: imports, config ====\n",
        "import os, io, gzip, math, random, unicodedata, urllib.request, zipfile\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# cc.* are larger and robust; you can switch to wiki.* if you prefer\n",
        "EN_URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
        "HI_URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\"\n",
        "\n",
        "# MUSE-style bilingual dictionary (one word pair per line: \"english hindi\")\n",
        "DICT_URL = \"https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\"\n",
        "\n",
        "#DATA_DIR = \"./data_xling\"\n",
        "#os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Keep vocab sizes modest for speed/RAM; adjust if you want stronger results\n",
        "MAX_VOCAB_EN = 120_000\n",
        "MAX_VOCAB_HI = 120_000\n",
        "\n",
        "# CSLS params\n",
        "CSLS_K = 10\n",
        "N_SRC_FOR_CSLS_R_T = 5000   # subsample of mapped source vectors to compute r_T\n",
        "\n",
        "# Batch size for similarity computations\n",
        "BATCH = 2048\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 1) Download files  ====\n",
        "import os, urllib.request\n",
        "\n",
        "def download(url, out_path):\n",
        "    if not os.path.exists(out_path):\n",
        "        print(f\"Downloading {url} -> {out_path}\")\n",
        "        urllib.request.urlretrieve(url, out_path)\n",
        "    else:\n",
        "        print(f\"Found {out_path}\")\n",
        "\n",
        "\n",
        "\n",
        "BASE_DATA = \"./data\"\n",
        "EXTERNAL_DIR = os.path.join(BASE_DATA, \"external\")\n",
        "os.makedirs(EXTERNAL_DIR, exist_ok=True)\n",
        "\n",
        "# Write downloads into ./data/external\n",
        "en_path   = os.path.join(EXTERNAL_DIR, os.path.basename(EN_URL))\n",
        "hi_path   = os.path.join(EXTERNAL_DIR, os.path.basename(HI_URL))\n",
        "dict_path = os.path.join(EXTERNAL_DIR, os.path.basename(DICT_URL))\n",
        "\n",
        "download(EN_URL, en_path)\n",
        "download(HI_URL, hi_path)\n",
        "download(DICT_URL, dict_path)\n",
        "\n",
        "print(\"Saved to:\", EXTERNAL_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5-VJWbBEw2u",
        "outputId": "21e4b933-680e-4771-c530-bfbbf1814b9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz -> ./data/external/cc.en.300.vec.gz\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz -> ./data/external/cc.hi.300.vec.gz\n",
            "Downloading https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt -> ./data/external/en-hi.txt\n",
            "Saved to: ./data/external\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 1) Download files ====\n",
        "'''\n",
        "def download(url, out_path):\n",
        "    if not os.path.exists(out_path):\n",
        "        print(f\"Downloading {url} -> {out_path}\")\n",
        "        urllib.request.urlretrieve(url, out_path)\n",
        "    else:\n",
        "        print(f\"Found {out_path}\")\n",
        "\n",
        "en_path = os.path.join(DATA_DIR, os.path.basename(EN_URL))\n",
        "hi_path = os.path.join(DATA_DIR, os.path.basename(HI_URL))\n",
        "dict_path = os.path.join(DATA_DIR, os.path.basename(DICT_URL))\n",
        "\n",
        "download(EN_URL, en_path)\n",
        "download(HI_URL, hi_path)\n",
        "download(DICT_URL, dict_path)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEtKbwidw-OH",
        "outputId": "4a3273da-5480-45d3-87c4-fc77793fd267"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz -> ./data_xling/cc.en.300.vec.gz\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz -> ./data_xling/cc.hi.300.vec.gz\n",
            "Downloading https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt -> ./data_xling/en-hi.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 2) Load fastText .vec(.gz) into numpy (top-N) ====\n",
        "def l2_normalize(M, eps=1e-8):\n",
        "    nrm = np.linalg.norm(M, axis=1, keepdims=True)\n",
        "    nrm = np.maximum(nrm, eps)\n",
        "    return M / nrm\n",
        "\n",
        "def try_parse_header(line):\n",
        "    # Some .vec have \"vocab dim\" in first line\n",
        "    parts = line.strip().split()\n",
        "    if len(parts) == 2:\n",
        "        try:\n",
        "            return int(parts[0]), int(parts[1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def load_vec_gz(path, max_vocab=None, dtype=np.float32, lowercase=False, normalize_unicode=True):\n",
        "    words, vecs = [], []\n",
        "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        first = f.readline()\n",
        "        # If first line is header, skip it\n",
        "        header = try_parse_header(first)\n",
        "        if header is None:\n",
        "            # first line is actually data; process it\n",
        "            parts = first.rstrip(\"\\n\").split(\" \")\n",
        "            w, vals = parts[0], parts[1:]\n",
        "            if lowercase: w = w.lower()\n",
        "            if normalize_unicode: w = unicodedata.normalize(\"NFC\", w)\n",
        "            words.append(w); vecs.append(np.array(vals, dtype=dtype))\n",
        "        # now rest\n",
        "        for i, line in enumerate(f, start=2):\n",
        "            if max_vocab is not None and len(words) >= max_vocab:\n",
        "                break\n",
        "            parts = line.rstrip(\"\\n\").split(\" \")\n",
        "            if len(parts) < 10:  # skip broken lines\n",
        "                continue\n",
        "            w, vals = parts[0], parts[1:]\n",
        "            if lowercase: w = w.lower()\n",
        "            if normalize_unicode: w = unicodedata.normalize(\"NFC\", w)\n",
        "            words.append(w); vecs.append(np.array(vals, dtype=dtype))\n",
        "    W = np.vstack(vecs)\n",
        "    return words, W\n",
        "\n",
        "print(\"Loading EN vectors...\")\n",
        "en_words, en_vecs = load_vec_gz(en_path, max_vocab=MAX_VOCAB_EN, lowercase=True)\n",
        "print(\"Loading HI vectors...\")\n",
        "hi_words, hi_vecs = load_vec_gz(hi_path, max_vocab=MAX_VOCAB_HI, lowercase=False)\n",
        "\n",
        "print(\"Shapes:\", en_vecs.shape, hi_vecs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZTJ-gGuxEC6",
        "outputId": "4af041e0-0525-4ceb-eb5d-671ca5bad2aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading EN vectors...\n",
            "Loading HI vectors...\n",
            "Shapes: (120000, 300) (120000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 3) Build word-to-index maps + normalize once ====\n",
        "en2i = {w:i for i,w in enumerate(en_words)}\n",
        "hi2i = {w:i for i,w in enumerate(hi_words)}\n",
        "\n",
        "# Pre-normalize to unit length (we'll also renormalize after mapping)\n",
        "en_vecs = l2_normalize(en_vecs)\n",
        "hi_vecs = l2_normalize(hi_vecs)\n"
      ],
      "metadata": {
        "id": "qaFrMvJWxHyt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 4) Load dictionary and filter to words we actually have ====\n",
        "pairs = []\n",
        "with io.open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if not line.strip(): continue\n",
        "        src, tgt = line.strip().split()\n",
        "        src = unicodedata.normalize(\"NFC\", src).lower()\n",
        "        tgt = unicodedata.normalize(\"NFC\", tgt)\n",
        "        if src in en2i and tgt in hi2i:\n",
        "            pairs.append((src, tgt))\n",
        "\n",
        "print(\"Total dictionary pairs present in our vocabs:\", len(pairs))\n",
        "\n",
        "# Shuffle and train/dev/test split (80/10/10)\n",
        "rng = np.random.default_rng(SEED)\n",
        "rng.shuffle(pairs)\n",
        "n = len(pairs)\n",
        "n_train = int(0.8 * n)\n",
        "n_dev   = int(0.1 * n)\n",
        "train_pairs = pairs[:n_train]\n",
        "dev_pairs   = pairs[n_train:n_train+n_dev]\n",
        "test_pairs  = pairs[n_train+n_dev:]\n",
        "\n",
        "print(len(train_pairs), len(dev_pairs), len(test_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-dY9d1kxLWp",
        "outputId": "345f7c85-216e-4940-b092-5d27901e6817"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dictionary pairs present in our vocabs: 24697\n",
            "19757 2469 2471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 5) Build aligned matrices for seed pairs ====\n",
        "def pairs_to_mats(pairs, src2i, tgt2i, src_vecs, tgt_vecs):\n",
        "    xs = np.stack([src_vecs[src2i[s]] for s,t in pairs], axis=0)\n",
        "    ys = np.stack([tgt_vecs[tgt2i[t]] for s,t in pairs], axis=0)\n",
        "    return xs, ys\n",
        "\n",
        "Xtr, Ytr = pairs_to_mats(train_pairs, en2i, hi2i, en_vecs, hi_vecs)\n",
        "Xdv, Ydv = pairs_to_mats(dev_pairs,   en2i, hi2i, en_vecs, hi_vecs)\n",
        "Xte, Yte = pairs_to_mats(test_pairs,  en2i, hi2i, en_vecs, hi_vecs)\n",
        "\n",
        "print(\"Train/dev/test matrices:\", Xtr.shape, Xdv.shape, Xte.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-30d4H0Sxf_2",
        "outputId": "489d1a79-cbdb-4b6a-b1bc-edc4fc18f156"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/dev/test matrices: (19757, 300) (2469, 300) (2471, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 6) Simple mappings ====\n",
        "def fit_lls(X, Y):\n",
        "    # Solve min_W ||XW - Y||_F via least squares\n",
        "    # lstsq handles rank issues gracefully\n",
        "    W, *_ = np.linalg.lstsq(X, Y, rcond=None)\n",
        "    return W  # shape (d, d)\n",
        "\n",
        "def fit_procrustes(X, Y):\n",
        "    # Solve min_W ||XW - Y|| with W orthonormal; W = U V^T, SVD of X^T Y\n",
        "    M = X.T @ Y\n",
        "    U, _, Vt = np.linalg.svd(M, full_matrices=False)\n",
        "    W = U @ Vt\n",
        "    return W\n",
        "\n",
        "# Learn mappings on train\n",
        "W_lls  = fit_lls(Xtr, Ytr)\n",
        "W_proc = fit_procrustes(Xtr, Ytr)\n"
      ],
      "metadata": {
        "id": "fFP4SfC7xkf_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 7) Retrieval helpers (cosine + CSLS) ====\n",
        "# Note: All vectors assumed L2-normalized.\n",
        "\n",
        "def cosine_topk(queries, targets, k=10, batch=BATCH):\n",
        "    # returns (indices, sims) for top-k targets per query\n",
        "    nQ = queries.shape[0]\n",
        "    top_idx = np.empty((nQ, k), dtype=np.int32)\n",
        "    top_sim = np.empty((nQ, k), dtype=np.float32)\n",
        "    for i in range(0, nQ, batch):\n",
        "        q = queries[i:i+batch]\n",
        "        S = q @ targets.T  # (b, Nt)\n",
        "        # partial top-k\n",
        "        idx = np.argpartition(-S, kth=k-1, axis=1)[:, :k]\n",
        "        part = np.take_along_axis(S, idx, axis=1)\n",
        "        # sort within the k\n",
        "        ordk = np.argsort(-part, axis=1)\n",
        "        idx = np.take_along_axis(idx, ordk, axis=1)\n",
        "        part = np.take_along_axis(part, ordk, axis=1)\n",
        "        top_idx[i:i+q.shape[0]] = idx\n",
        "        top_sim[i:i+q.shape[0]] = part\n",
        "    return top_idx, top_sim\n",
        "\n",
        "def mean_topk_cos_to_A_per_target(B, A, k=CSLS_K, batch=BATCH):\n",
        "    # r_T(y) = mean of top-k cos(y, A) for each y in B\n",
        "    Nt = B.shape[0]\n",
        "    rT = np.empty(Nt, dtype=np.float32)\n",
        "    for i in range(0, Nt, batch):\n",
        "        b = B[i:i+batch]\n",
        "        S = b @ A.T  # (b, Na)\n",
        "        idx = np.argpartition(-S, kth=k-1, axis=1)[:, :k]\n",
        "        part = np.take_along_axis(S, idx, axis=1)\n",
        "        rT[i:i+b.shape[0]] = part.mean(axis=1)\n",
        "    return rT\n",
        "\n",
        "def csls_scores(queries, targets, rT, k=CSLS_K, batch=BATCH):\n",
        "    # returns full CSLS score matrices in batches (to pick argmax/argtopk)\n",
        "    # For memory, we compute per-batch and return top-k like cosine_topk\n",
        "    nQ = queries.shape[0]\n",
        "    k_out = 10  # we want at least up to P@10\n",
        "    top_idx = np.empty((nQ, k_out), dtype=np.int32)\n",
        "    top_scr = np.empty((nQ, k_out), dtype=np.float32)\n",
        "    for i in range(0, nQ, batch):\n",
        "        q = queries[i:i+batch]\n",
        "        # r_S(q): mean of top-k cos(q, targets)\n",
        "        S = q @ targets.T  # (b, Nt)\n",
        "        idx_q = np.argpartition(-S, kth=k-1, axis=1)[:, :k]\n",
        "        part_q = np.take_along_axis(S, idx_q, axis=1)\n",
        "        rS = part_q.mean(axis=1, keepdims=True)  # (b,1)\n",
        "\n",
        "        # CSLS: 2*cos - rT - rS\n",
        "        CS = 2.0 * S - rT[None, :] - rS  # broadcast\n",
        "        idx = np.argpartition(-CS, kth=k_out-1, axis=1)[:, :k_out]\n",
        "        part = np.take_along_axis(CS, idx, axis=1)\n",
        "        ordk = np.argsort(-part, axis=1)\n",
        "        idx = np.take_along_axis(idx, ordk, axis=1)\n",
        "        part = np.take_along_axis(part, ordk, axis=1)\n",
        "        top_idx[i:i+q.shape[0]] = idx\n",
        "        top_scr[i:i+q.shape[0]] = part\n",
        "    return top_idx, top_scr\n"
      ],
      "metadata": {
        "id": "f8NEsSFFxowG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 8) BLI evaluation (P@1/5/10, MRR) ====\n",
        "def compute_metrics(topk_idx, gold_tgt_idx, k_list=(1,5,10)):\n",
        "    # gold_tgt_idx: shape (nQ,), each the index in the target vocab\n",
        "    nQ = len(gold_tgt_idx)\n",
        "    hits = {k:0 for k in k_list}\n",
        "    rr_sum = 0.0\n",
        "    for i in range(nQ):\n",
        "        pred = topk_idx[i]\n",
        "        # find rank of gold\n",
        "        # where does gold appear?\n",
        "        where = np.where(pred == gold_tgt_idx[i])[0]\n",
        "        if where.size > 0:\n",
        "            rank = int(where[0]) + 1\n",
        "        else:\n",
        "            rank = None\n",
        "        if rank is not None:\n",
        "            for k in k_list:\n",
        "                if rank <= k:\n",
        "                    hits[k] += 1\n",
        "            rr_sum += 1.0 / rank\n",
        "        else:\n",
        "            # no hit within returned top-k; MRR contributes 0\n",
        "            pass\n",
        "    res = {f\"P@{k}\": hits[k]/nQ for k in k_list}\n",
        "    res[\"MRR\"] = rr_sum / nQ\n",
        "    return res\n",
        "\n",
        "# Helper to prepare test queries and gold target indices\n",
        "def test_queries_and_gold(test_pairs, src2i, tgt2i, src_vecs, tgt_vecs, W=None):\n",
        "    # map EN -> HI\n",
        "    Xq = np.stack([src_vecs[src2i[s]] for s,t in test_pairs], axis=0)\n",
        "    if W is not None:\n",
        "        Xq = Xq @ W\n",
        "        Xq = l2_normalize(Xq)\n",
        "    gold = np.array([tgt2i[t] for s,t in test_pairs], dtype=np.int32)\n",
        "    return Xq, gold\n"
      ],
      "metadata": {
        "id": "m8ncG8c_xwrx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 9) Run BLI for the methods and scorers ====\n",
        "\n",
        "# Target matrix for retrieval\n",
        "TGT = hi_vecs  # (Nt, d) already normalized\n",
        "\n",
        "# Prepare test queries per method\n",
        "methods = {\n",
        "    \"NoMap\": None,               # use EN as-is (will perform poorly; baseline)\n",
        "    \"LLS\": W_lls,                # learned linear map\n",
        "    \"Procrustes\": W_proc,        # orthogonal map\n",
        "}\n",
        "\n",
        "# Optional \"NormOnly\" baseline is essentially the same as \"NoMap\" here,\n",
        "# since we already normalized both spaces. If you want it explicitly:\n",
        "# methods[\"NormOnly\"] = None\n",
        "\n",
        "results = []\n",
        "\n",
        "# Precompute r_T for CSLS once per method (depends on mapped source distribution)\n",
        "def compute_rT_for_method(W):\n",
        "    # Use a subset of mapped train EN vectors to define neighbor density\n",
        "    Xmap = Xtr if W is None else l2_normalize(Xtr @ W)\n",
        "    if len(Xmap) > N_SRC_FOR_CSLS_R_T:\n",
        "        Xmap = Xmap[:N_SRC_FOR_CSLS_R_T]\n",
        "    return mean_topk_cos_to_A_per_target(TGT, Xmap, k=CSLS_K, batch=BATCH)\n",
        "\n",
        "for name, W in methods.items():\n",
        "    print(f\"\\n=== Method: {name} ===\")\n",
        "    # Build test queries\n",
        "    Xq, gold = test_queries_and_gold(test_pairs, en2i, hi2i, en_vecs, hi_vecs, W=W)\n",
        "\n",
        "    # Cosine retrieval\n",
        "    top_idx_cos, _ = cosine_topk(Xq, TGT, k=10, batch=BATCH)\n",
        "    m_cos = compute_metrics(top_idx_cos, gold)\n",
        "    print(\"Cosine:\", m_cos)\n",
        "\n",
        "    # CSLS retrieval\n",
        "    rT = compute_rT_for_method(W)\n",
        "    top_idx_csls, _ = csls_scores(Xq, TGT, rT, k=CSLS_K, batch=BATCH)\n",
        "    m_csls = compute_metrics(top_idx_csls, gold)\n",
        "    print(\"CSLS:\", m_csls)\n",
        "\n",
        "    results.append((name, m_cos, m_csls))\n",
        "\n",
        "# Print compact table\n",
        "print(\"\\n=== Summary (P@1 / P@5 / P@10 / MRR) ===\")\n",
        "for name, m_cos, m_csls in results:\n",
        "    def fmt(m): return f\"{m['P@1']:.3f} / {m['P@5']:.3f} / {m['P@10']:.3f} / {m['MRR']:.3f}\"\n",
        "    print(f\"{name:12s}  COS  {fmt(m_cos)}    |   CSLS {fmt(m_csls)}\")\n",
        "\n",
        "# Coverage =\n",
        "# fraction of dictionary pairs present is shown earlier (\"Total dictionary pairs present...\").\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emfY3wpwxsNz",
        "outputId": "7b4aca36-d8e2-43d3-f79b-40e23555cde5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Method: NoMap ===\n",
            "Cosine: {'P@1': 0.0, 'P@5': 0.0, 'P@10': 0.0, 'MRR': 0.0}\n",
            "CSLS: {'P@1': 0.0, 'P@5': 0.0, 'P@10': 0.0, 'MRR': 0.0}\n",
            "\n",
            "=== Method: LLS ===\n",
            "Cosine: {'P@1': 0.11493322541481182, 'P@5': 0.2630513961958721, 'P@10': 0.34075273168757586, 'MRR': 0.17958782190874453}\n",
            "CSLS: {'P@1': 0.20518008903278026, 'P@5': 0.4196681505463375, 'P@10': 0.49575070821529743, 'MRR': 0.2947640245900063}\n",
            "\n",
            "=== Method: Procrustes ===\n",
            "Cosine: {'P@1': 0.17280453257790368, 'P@5': 0.3597733711048159, 'P@10': 0.43261837312828816, 'MRR': 0.2524013476967745}\n",
            "CSLS: {'P@1': 0.21448806151355726, 'P@5': 0.41278834479967624, 'P@10': 0.4925131525698098, 'MRR': 0.30003276097974635}\n",
            "\n",
            "=== Summary (P@1 / P@5 / P@10 / MRR) ===\n",
            "NoMap         COS  0.000 / 0.000 / 0.000 / 0.000    |   CSLS 0.000 / 0.000 / 0.000 / 0.000\n",
            "LLS           COS  0.115 / 0.263 / 0.341 / 0.180    |   CSLS 0.205 / 0.420 / 0.496 / 0.295\n",
            "Procrustes    COS  0.173 / 0.360 / 0.433 / 0.252    |   CSLS 0.214 / 0.413 / 0.493 / 0.300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A) Coverage & quick stats\n",
        "total_pairs_raw = sum(1 for _ in open(dict_path, encoding=\"utf-8\"))\n",
        "total_pairs_kept = len(train_pairs) + len(dev_pairs) + len(test_pairs)\n",
        "print(f\"Dictionary lines (raw): {total_pairs_raw}\")\n",
        "print(f\"Pairs kept (in-vocab): {total_pairs_kept} ({100*total_pairs_kept/max(1,total_pairs_raw):.1f}%)\")\n",
        "print(f\"Train/Dev/Test sizes: {len(train_pairs)}/{len(dev_pairs)}/{len(test_pairs)}\")\n",
        "print(f\"EN vocab used: {len(en_words)}  |  HI vocab used: {len(hi_words)}\")\n"
      ],
      "metadata": {
        "id": "ECqufNCbypJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faea13e2-2692-473b-eb02-04d4d68e5a88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary lines (raw): 38221\n",
            "Pairs kept (in-vocab): 24697 (64.6%)\n",
            "Train/Dev/Test sizes: 19757/2469/2471\n",
            "EN vocab used: 120000  |  HI vocab used: 120000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B) Nearest neighbors (EN→HI) with COS or CSLS\n",
        "from collections import defaultdict\n",
        "gold_hi_by_en = defaultdict(list)\n",
        "for s,t in (train_pairs + dev_pairs + test_pairs):\n",
        "    gold_hi_by_en[s].append(t)\n",
        "\n",
        "def _compute_rT_for(method_name):\n",
        "    W = methods[method_name]\n",
        "    Xmap = Xtr if W is None else l2_normalize(Xtr @ W)\n",
        "    Xsub = Xmap[:min(5000, len(Xmap))]\n",
        "    return mean_topk_cos_to_A_per_target(hi_vecs, Xsub, k=CSLS_K, batch=BATCH)\n",
        "\n",
        "_rT_cache = {}\n",
        "\n",
        "def show_neighbors_en2hi(word_en, method=\"Procrustes\", scorer=\"csls\", k=10):\n",
        "    w = word_en.strip().lower()\n",
        "    if w not in en2i:\n",
        "        print(f\"'{word_en}' not in EN vocab.\"); return\n",
        "    x = en_vecs[en2i[w]][None,:]\n",
        "    W = methods[method]\n",
        "    if W is not None:\n",
        "        x = l2_normalize(x @ W)\n",
        "    if scorer.lower() == \"cos\":\n",
        "        idx, sims = cosine_topk(x, hi_vecs, k=k, batch=BATCH)\n",
        "        scores = sims[0]\n",
        "    else:\n",
        "        if method not in _rT_cache:\n",
        "            _rT_cache[method] = _compute_rT_for(method)\n",
        "        idx, scr = csls_scores(x, hi_vecs, _rT_cache[method], k=CSLS_K, batch=BATCH)\n",
        "        scores = scr[0]\n",
        "    preds = [hi_words[j] for j in idx[0]]\n",
        "    print(f\"\\nEN: {w}\")\n",
        "    print(\"Gold HI:\", gold_hi_by_en[w] if gold_hi_by_en[w] else \"(none)\")\n",
        "    print(f\"Top-{k} ({method}, {scorer.upper()}):\")\n",
        "    for r,(tok,sc) in enumerate(zip(preds, scores), 1):\n",
        "        mark = \" <- GOLD\" if tok in gold_hi_by_en[w] else \"\"\n",
        "        print(f\"{r:2d}. {tok:20s}  {sc:.4f}{mark}\")\n",
        "\n",
        "# Examples:\n",
        "show_neighbors_en2hi(\"river\", method=\"Procrustes\", scorer=\"csls\", k=10)\n",
        "show_neighbors_en2hi(\"computer\", method=\"LLS\", scorer=\"cos\", k=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEuL0SHB2E7g",
        "outputId": "e2c7f65f-96f7-4bc7-921b-2035af1947fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EN: river\n",
            "Gold HI: ['नदी']\n",
            "Top-10 (Procrustes, CSLS):\n",
            " 1. नदी                   0.1804 <- GOLD\n",
            " 2. किनारे                0.1739\n",
            " 3. समीप                  0.1043\n",
            " 4. घाट                   0.0303\n",
            " 5. गंगा                  0.0288\n",
            " 6. रिवर                  0.0275\n",
            " 7. नदिया                 0.0222\n",
            " 8. झील                   0.0186\n",
            " 9. जलप्रपात              0.0181\n",
            "10. ज्वारनदीमुख           0.0044\n",
            "\n",
            "EN: computer\n",
            "Gold HI: ['कंप्यूटर', 'कम्प्यूटर', 'संगणक']\n",
            "Top-10 (LLS, COS):\n",
            " 1. कंप्यूटर              0.7457 <- GOLD\n",
            " 2. सॉफ्टवेयर             0.6296\n",
            " 3. आभूषणपुस्तकेंकंप्यूटर  0.5972\n",
            " 4. मोबाइल                0.5827\n",
            " 5. कम्प्यूटर             0.5810 <- GOLD\n",
            " 6. कम्पयूटर              0.5748\n",
            " 7. तकनीकी                0.5732\n",
            " 8. कंप्यूट               0.5695\n",
            " 9. ख़बरट्रैवलफोटोशब्दकोशज्योतिषस्वास्थ्यगानेसिनेमाकूपनहिन्दी  0.5590\n",
            "10. इंटरनेट               0.5557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C) Inspect a test word: where does gold land?\n",
        "def _rank_of(pred_indices, gold_index):\n",
        "    pos = np.where(pred_indices == gold_index)[0]\n",
        "    return int(pos[0])+1 if pos.size else None\n",
        "\n",
        "def inspect_test_word(src_en, method=\"Procrustes\", scorer=\"csls\", k=10):\n",
        "    w = src_en.strip().lower()\n",
        "    if (w, ) not in [(s,) for s,_ in test_pairs]:\n",
        "        print(f\"'{w}' not in test set.\"); return\n",
        "    gold_hi = [t for s,t in test_pairs if s==w][0]\n",
        "    x = en_vecs[en2i[w]][None,:]\n",
        "    W = methods[method]\n",
        "    if W is not None:\n",
        "        x = l2_normalize(x @ W)\n",
        "    if scorer.lower()==\"cos\":\n",
        "        idx, sims = cosine_topk(x, hi_vecs, k=k, batch=BATCH)\n",
        "    else:\n",
        "        if method not in _rT_cache:\n",
        "            _rT_cache[method] = _compute_rT_for(method)\n",
        "        idx, sims = csls_scores(x, hi_vecs, _rT_cache[method], k=CSLS_K, batch=BATCH)\n",
        "    preds = [hi_words[j] for j in idx[0]]\n",
        "    r = _rank_of(idx[0], hi2i[gold_hi])\n",
        "    print(f\"\\nTest word EN: {w}  | GOLD HI: {gold_hi}  | Rank: {r}\")\n",
        "    for i,(tok,sc) in enumerate(zip(preds, sims[0]), 1):\n",
        "        print(f\"{i:2d}. {tok:20s} {sc:.4f}{' <- GOLD' if tok==gold_hi else ''}\")\n",
        "\n",
        "# Example:\n",
        "inspect_test_word(\"River\", method=\"Procrustes\", scorer=\"csls\", k=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paNTZ2uX2P4j",
        "outputId": "4812aef8-a976-4901-9b9d-bb86aafbdcaf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'river' not in test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable a font that supports Devanagari (Hindi) in Colab\n",
        "!wget -q https://github.com/google/fonts/raw/main/ofl/notosansdevanagari/NotoSansDevanagari-Regular.ttf -O /usr/share/fonts/truetype/NotoSansDevanagari-Regular.ttf\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams['font.family'] = 'Noto Sans Devanagari'\n"
      ],
      "metadata": {
        "id": "vl1leXkG25ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E) HI→EN mapping + evaluation (minimal)\n",
        "def fit_lls(X, Y):  # (re)define tiny helper here for clarity\n",
        "    W, *_ = np.linalg.lstsq(X, Y, rcond=None); return W\n",
        "\n",
        "def fit_procrustes(X, Y):\n",
        "    U, _, Vt = np.linalg.svd(X.T @ Y, full_matrices=False); return U @ Vt\n",
        "\n",
        "# Train HI->EN mappings on the same pairs\n",
        "W_lls_hi2en  = fit_lls(Ytr, Xtr)\n",
        "W_proc_hi2en = fit_procrustes(Ytr, Xtr)\n",
        "methods_hi2en = {\"NoMap\": None, \"LLS\": W_lls_hi2en, \"Procrustes\": W_proc_hi2en}\n",
        "\n",
        "def test_queries_and_gold_hi2en(pairs_):\n",
        "    Xq = np.stack([hi_vecs[hi2i[t]] for s,t in pairs_], axis=0)\n",
        "    gold = np.array([en2i[s] for s,t in pairs_], dtype=np.int32)\n",
        "    return Xq, gold\n",
        "\n",
        "def run_bli_hi2en():\n",
        "    TGT = en_vecs\n",
        "    for name, W in methods_hi2en.items():\n",
        "        Xq, gold = test_queries_and_gold_hi2en(test_pairs)\n",
        "        if W is not None: Xq = l2_normalize(Xq @ W)\n",
        "        # cosine\n",
        "        top_idx_cos, _ = cosine_topk(Xq, TGT, k=10, batch=BATCH)\n",
        "        m_cos = compute_metrics(top_idx_cos, gold)\n",
        "        # csls\n",
        "        Xmap = Ytr if W is None else l2_normalize(Ytr @ W)\n",
        "        rT = mean_topk_cos_to_A_per_target(TGT, Xmap[:min(5000,len(Xmap))], k=CSLS_K, batch=BATCH)\n",
        "        top_idx_csls, _ = csls_scores(Xq, TGT, rT, k=CSLS_K, batch=BATCH)\n",
        "        m_csls = compute_metrics(top_idx_csls, gold)\n",
        "        print(f\"{name:11s}  COS {m_cos}  |  CSLS {m_csls}\")\n",
        "\n",
        "# Run:\n",
        "run_bli_hi2en()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "satstB333Yb0",
        "outputId": "3a090535-f817-4ae4-ae0f-0bd800357e1e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NoMap        COS {'P@1': 0.0, 'P@5': 0.0, 'P@10': 0.0, 'MRR': 0.0}  |  CSLS {'P@1': 0.0, 'P@5': 0.0, 'P@10': 0.0, 'MRR': 0.0}\n",
            "LLS          COS {'P@1': 0.17280453257790368, 'P@5': 0.35127478753541075, 'P@10': 0.4087414002428167, 'MRR': 0.24750342063170902}  |  CSLS {'P@1': 0.2237960339943343, 'P@5': 0.44071226224200727, 'P@10': 0.5240793201133145, 'MRR': 0.3157580312578293}\n",
            "Procrustes   COS {'P@1': 0.18251719951436665, 'P@5': 0.3986240388506678, 'P@10': 0.48725212464589235, 'MRR': 0.2741069421158456}  |  CSLS {'P@1': 0.17280453257790368, 'P@5': 0.3925536220153784, 'P@10': 0.48927559692432215, 'MRR': 0.26594672807744457}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F) Tiny sentence retrieval (toy parallel)\n",
        "RUN_SENT_RETRIEVAL = True\n",
        "if RUN_SENT_RETRIEVAL:\n",
        "    en_sents = [\n",
        "        \"the river flows near the village\",\n",
        "        \"students study computer science\",\n",
        "        \"a green forest surrounds the lake\",\n",
        "        \"she bought fresh vegetables from the market\",\n",
        "        \"the weather is pleasant today\",\n",
        "    ]\n",
        "    hi_sents = [\n",
        "        \"नदी गाँव के पास बहती है\",\n",
        "        \"छात्र कंप्यूटर विज्ञान पढ़ते हैं\",\n",
        "        \"एक हरा जंगल झील को घेरता है\",\n",
        "        \"उसने बाजार से ताज़ी सब्जियाँ खरीदीं\",\n",
        "        \"आज मौसम सुहावना है\",\n",
        "    ]\n",
        "    def sent_vec_en(s):\n",
        "        idx = [en2i.get(w, None) for w in s.lower().split()]\n",
        "        idx = [i for i in idx if i is not None]\n",
        "        return np.mean(en_vecs[idx], axis=0) if idx else np.zeros(en_vecs.shape[1])\n",
        "    def sent_vec_hi(s):\n",
        "        idx = [hi2i.get(w, None) for w in s.split()]\n",
        "        idx = [i for i in idx if i is not None]\n",
        "        return np.mean(hi_vecs[idx], axis=0) if idx else np.zeros(hi_vecs.shape[1])\n",
        "\n",
        "    EN = np.stack([sent_vec_en(s) for s in en_sents])\n",
        "    HI = np.stack([sent_vec_hi(s) for s in hi_sents])\n",
        "    EN_map = l2_normalize(EN @ W_proc)   # map EN→HI using Procrustes you learned\n",
        "\n",
        "    idx, _ = cosine_topk(EN_map, HI, k=1, batch=64)\n",
        "    hits1 = (idx[:,0] == np.arange(len(hi_sents))).mean()\n",
        "    print(f\"Sentence retrieval Recall@1 (toy): {hits1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otES9mwA3e0d",
        "outputId": "d5dd5056-06da-4e2c-ac83-c192776ddf70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence retrieval Recall@1 (toy): 0.80\n"
          ]
        }
      ]
    }
  ]
}